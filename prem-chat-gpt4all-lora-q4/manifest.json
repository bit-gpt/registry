{
    "id": "gpt4all-lora-q4",
    "name": "GPT4ALL-Lora Q4",
    "description": "GPT4ALL-Lora Q4",
    "documentation": "",
    "modelInfo": {
        "maxLength": 12000,
        "tokenLimit": 4000,
        "weightsName": "vicuna-7b-q4.bin",
        "weightsSize": 4212859520,
        "devices": [
            "m1"
        ],
        "memoryRequirements": "8gb",
        "inferenceTime": "5 tokens per second"
    },
    "apps": [
        "chat",
        "embeddings"
    ],
    "dockerImage": "ghcr.io/premai-io/prem-chat-gpt4all-lora-q4-m1:latest",
    "defaultPort": 8002
}
