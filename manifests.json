[{"id": "qdrant", "name": "Qdrant", "description": "Qdrant Vectorstore", "documentation": "<h1>Documentation</h1>\n<h2>Description</h2>\n<h2>Hardware Requirements</h2>\n<h2>Example Usage</h2>\n<h2>Fine Tuning Instructions &amp; Cost</h2>\n<h2>Inference Benchmarks</h2>", "apps": ["store"], "icon": "/logos/prem-store-qdrant/logo.svg", "modelInfo": {}, "dockerImage": "qdrant/qdrant:v1.0.3", "defaultPort": 6333}, {"id": "redis", "name": "Redis", "description": "Redis Vectorstore", "documentation": "<h1>Documentation</h1>\n<h2>Description</h2>\n<h2>Hardware Requirements</h2>\n<h2>Example Usage</h2>\n<h2>Fine Tuning Instructions &amp; Cost</h2>\n<h2>Inference Benchmarks</h2>", "apps": ["store"], "icon": "/logos/prem-store-redis/logo.svg", "modelInfo": {}, "dockerImage": "redis/redis-stack-server:latest", "defaultPort": 6379}, {"id": "vicuna-7b-q4", "name": "Vicuna 7B Q4", "description": "Vicuna 7B Q4", "documentation": "<h1>Documentation</h1>\n<h2>Description</h2>\n<h2>Hardware Requirements</h2>\n<h2>Example Usage</h2>\n<h2>Fine Tuning Instructions &amp; Cost</h2>\n<h2>Inference Benchmarks</h2>", "icon": "/logos/prem-chat-vicuna-7b-q4/logo.svg", "modelInfo": {"maxLength": 12000, "tokenLimit": 4000, "weightsName": "vicuna-7b-q4.bin", "weightsSize": 4212859520, "devices": ["m1"], "memoryRequirements": "8gb", "inferenceTime": "5 tokens per second"}, "apps": ["chat", "embeddings"], "dockerImage": "ghcr.io/premai-io/prem-chat-vicuna-7b-q4-m1:latest", "defaultPort": 8001}, {"id": "gpt4all-lora-q4", "name": "GPT4ALL-Lora Q4", "icon": "/logos/prem-chat-gpt4all-lora-q4/logo.svg", "description": "GPT4ALL-Lora Q4", "documentation": "<h1>Documentation</h1>\n<h2>Description</h2>\n<h2>Hardware Requirements</h2>\n<h2>Example Usage</h2>\n<h2>Fine Tuning Instructions &amp; Cost</h2>\n<h2>Inference Benchmarks</h2>", "modelInfo": {"maxLength": 12000, "tokenLimit": 4000, "weightsName": "vicuna-7b-q4.bin", "weightsSize": 4212859520, "devices": ["m1"], "memoryRequirements": "8gb", "inferenceTime": "5 tokens per second"}, "apps": ["chat", "embeddings"], "dockerImage": "ghcr.io/premai-io/prem-chat-gpt4all-lora-q4-m1:latest", "defaultPort": 8002}]
