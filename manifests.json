[{"id": "llama-2-7b-chat", "name": "Llama v2 7B Chat", "beta": true, "description": "Llama v2 7B Chat, developed by Meta, is an auto-regressive language model that uses an optimized transformer architecture. It's a 7B parameters finetuned Chat model version using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. It's released under a custom commercial license.", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nLlama V2 7B Chat, developed by Meta, is an auto-regressive language model that uses an optimized transformer architecture. It's a 7B parameters finetuned Chat model version using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. It's released under a custom commercial license and its inference can be run on various GPU configurations. <a href='https://huggingface.co/meta-llama/Llama-2-7b-chat' target='_blank'>Learn More</a>\n\n## \ud83d\udcbb Hardware Requirements\n\n> **Memory requirements**: 15.01 GB (14318 MiB).\n\nTo run the `llama-2-7b-chat` service, you'll need the following hardware configuration:\n\n### Cloud Platforms\n\nIf you are using AWS:\n\n- Instance Type: `p3.2xlarge` or higher\n- GPU: NVIDIA A100, NVIDIA V100\n\nIf you are using Paperspace:\n\n- Instance Type: `V100` or higher\n- GPU: NVIDIA A100, NVIDIA V100\n\n### On-Premise Platforms\n\nYou'll need access to a GPU with the following options:\n- A100 GPUs: A100 GPUs are preferred for training all model sizes.\n- V100 GPUs: The model can be run on V100 GPUs.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> \\n\\nMachine learning models are typically trained on large datasets that are too large or too sensitive to be stored or transmitted securely over the internet. Running machine learning models on-premise allows you to keep your data local and avoid any potential security risks associated with transmitting sensitive data over the internet. Additionally, running machine learning models on-premise can provide better performance and faster training times, as you don't have to rely on internet connectivity.\\n\\n\\nsources:\\n\\n1. \\\"Why Run Machine Learning Mod\n\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> \\n\\nI have a few models that I have trained using TensorFlow and PyTorch, and I would like to run them on-premise, without using any cloud services. How can I do this?\\n\\nOption 1: Install TensorFlow and PyTorch on your local machine\\n\\nTo run your models on-premise, you can install TensorFlow and PyTorch on your local machine. Here are the steps for installing TensorFlow and PyTorch on a Windows, macOS, or Linux machine:\\n\\n\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> \\n\\nAnswer: Running machine learning models on-premise has several limitations, including:\\n\\n1. Data Security: By running your models on-premise, you may be exposing your data to potential security threats, such as data breaches or cyber attacks.\\n2. Limited Scalability: On-premise models may not be able to handle large datasets or high volumes of traffic, which can limit their scalability.\\n3. Difficulty in Managing Model Updates: Keeping your models up\n\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n<blockquote>\n\n \\n\\nThere are several trade-offs to consider when deciding whether to deploy machine learning models on-premise or in the cloud:\\n\\n1. Control and Security: On-premise deployment provides greater control over the underlying infrastructure, which can be critical for sensitive or regulated industries. However, this control comes at the cost of increased security and maintenance responsibilities.\\n2. Cost: Deploying models on-premise can require significant upfront costs for hardware and infrastructure,\n\n</blockquote>\n\nIt's visible above from the outputs that model sometimes tends just do text completion and not respond in a QnA manner. We recommend users of Llama-V2-7B-Chat to develop guardrails and to take appropriate precautions for any production use as it's only a text generation model by default.\n\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8228/v1\", max_tokens=4096)\nmessages = [HumanMessage(content=\"What are the trade-offs of deploying models on-premise I should be aware of?\")]\nprint(chat(messages))\n```\n\n> To know more on how to handle multi-turn conversation prompts specially for Llama-v2, check out: https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n\n\n### \ud83d\udeab Limitations and Biases\n\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide released by <a href='https://ai.meta.com/llama/responsible-use-guide/' target='_blank'>Meta here.</a>\n\n## \ud83d\udcdc License\nIt is made available under a custom commercial license, which is available <a href='https://ai.meta.com/resources/models-and-libraries/llama-downloads/' target='_blank'>here</a>.", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/chat-llama-2-7b-chat/logo.svg", "modelInfo": {"memoryRequirements": 14318, "tokensPerSecond": 44}, "interfaces": ["chat"], "dockerImages": {"gpu": {"size": 33304486337, "image": "ghcr.io/premai-io/chat-llama-2-7b-chat-gpu:1.0.1"}}, "defaultPort": 8000, "defaultExternalPort": 8228, "banner": null}, {"id": "stable-diffusion-x4-upscaler", "name": "Stable Diffusion x4 Upscaler", "description": "Stable Diffusion x4 upscaler is a text-guided latent upscaling diffusion model. It is trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nStable Diffusion x4 Upscaler is a diffusion model trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model. <a href='https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `stable-diffusion-x4-upscaler` service on Prem, you'll need access to a GPU with at least 16GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n\n### Before:\n![iron_man_image](https://github.com/premAI-io/prem-registry/assets/35634788/4557aa21-3707-4ee0-a92c-280d8158bd62)\n\n### After:\n![iron_man_docker](https://github.com/premAI-io/prem-registry/assets/35634788/fc661788-7805-49c7-bb18-938918bf30b7)\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with making requests locally\n\nThe service exposes the `/images/upscale` endpoint which can be used to upscale images. The following example shows how to:\n\n```python\nimport requests\nimport io\nimport base64\nfrom PIL import Image\n\nurl = 'http://localhost:8996/v1/images/upscale'\nfiles = {'image': open('iron_man_image.png', 'rb')}  #assuming we have an avg resolution quality iron man image here\ndata = {\n    'prompt': \"Super high resolution image of iron man, highly detailed, real life.\",\n    'n': 1,\n    'guidance_scale': 8\n}\n\nresponse = requests.post(url, files=files, data=data)\nimage_string = response.json()[\"data\"][0][\"b64_json\"]\n\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"iron_man_highres.png\", \"PNG\")\n\n```\n\nOr curl equivalent:\n```bash\ncurl -X POST http://localhost:8996/v1/images/upscale \\\n    -F \"image=@iron_man_image.png\" \\\n    -F \"prompt=Super high resolution image of iron man, highly detailed, real life.\" \\\n    -F \"n=1\" \\\n    -F \"guidance_scale=8\" \\\n    | jq -r '.data[0].b64_json' | base64 -d > iron_man_highres.png\n```\n\n## \ud83d\udcdc License\n\nThe model is under CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/upscaler-stable-diffusion-x4/logo.svg", "modelInfo": {"memoryRequirements": 2654, "secondsPerImage": 2}, "interfaces": ["upscaler"], "dockerImages": {"gpu": {"size": 24977428837, "image": "ghcr.io/premai-io/upscaler-stable-diffusion-x4-gpu:1.0.3"}}, "defaultPort": 8000, "defaultExternalPort": 8996, "banner": null}, {"id": "stable-diffusion-x2-latent-upscaler", "name": "Stable Diffusion x2 Latent Upscaler", "description": "Stable Diffusion x2 latent upscaler is a sophisticated diffusion model that operates in the same latent space as the Stable Diffusion model, which is decoded into a full-resolution image. This upscaling model is designed explicitely for Stable Diffusion and works for images generated by other Stable Diffusion models. It is developed by Katherine Crowson in collaboration with Stability AI.", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nStable Diffusion x2 Latent Upscaler is a diffusion-based upscaler developed by Katherine Crowson in collaboration with Stability AI. This model was trained on a high-resolution subset of the LAION-2B dataset. It is a diffusion model that operates in the same latent space as the Stable Diffusion model, which is decoded into a full-resolution image.<a href='https://huggingface.co/stabilityai/sd-x2-latent-upscaler' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `stable-diffusion-x2-latent-upscaler` service on Prem, you'll need access to a GPU with at least 16GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n### Before:\n![iron_man_image](https://github.com/premAI-io/prem-registry/assets/35634788/83122d2a-6951-4c66-afc1-f46a76472640)\n\n\n### After\n![iron_man_docker](https://github.com/premAI-io/prem-registry/assets/35634788/b3b41d3a-d62f-47d0-bedb-8564f7f39882)\n\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with making requests locally\n\nThe service exposes the `/images/upscale` endpoint which can be used to upscale images. The following example shows how to:\n\n```python\nimport requests\nimport io\nimport base64\nfrom PIL import Image\n\nurl = 'http://localhost:8997/v1/images/upscale'\nfiles = {'image': open('iron_man_image.png', 'rb')}  #assuming we have an avg resolution quality iron man image here\ndata = {\n    'prompt': \"Super high resolution image of iron man, highly detailed, real life.\",\n    'n': 1,\n    'guidance_scale': 8\n}\n\nresponse = requests.post(url, files=files, data=data)\nimage_string = response.json()[\"data\"][0][\"b64_json\"]\n\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"iron_man_highres.png\", \"PNG\")\n\n```\n\nOr curl equivalent:\n```bash\ncurl -X POST http://localhost:8997/v1/images/upscale \\\n    -F \"image=@iron_man_image.png\" \\\n    -F \"prompt=Super high resolution image of iron man, highly detailed, real life.\" \\\n    -F \"n=1\" \\\n    -F \"guidance_scale=8\" \\\n   | jq -r '.data[0].b64_json' | base64 -d > iron_man_highres.png\n```\n\n## :no_entry_sign: Limitations\nThis upscaler model only works with images generated using other stable diffusion models. To hack around making it work with any image, one can resize the image to `(2^x, 2^x)` where `x` can be any number and then feed the upscaler. Though the upscaling quality gains with this method is negligible.\n\n## \ud83d\udcdc License\n\nThe model is under CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/upscaler-stable-diffusion-x2-latent/logo.svg", "modelInfo": {"memoryRequirements": 2136, "secondsPerImage": 2}, "interfaces": ["upscaler"], "dockerImages": {"gpu": {"size": 23818054038, "image": "ghcr.io/premai-io/upscaler-stable-diffusion-x2-latent-gpu:1.0.3"}}, "defaultPort": 8000, "defaultExternalPort": 8997, "banner": null}, {"id": "mistral-7b-instruct", "name": "Mistral 7B Instruct", "beta": true, "description": "", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\nThe Mistral 7B Instruct is a model built by finetuning  <a href='https://huggingface.co/mistralai/Mistral-7B-v0.1' target='_blank'>Mistral-7B-v0.1</a>.\n\n## \ud83d\udcd2 Example Usage\n\nFollowing are a few example generations with `max_token=64` set.\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> 1. Security and Privacy: On-premise machine learning allows organizations to maintain full control over their data, ensuring that sensitive information is not compromised during transmission or storage. This can be particularly important for industries such as healthcare or finance where data privacy regulations are strict.\\n2. Customization: On-p\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> 1. Install the necessary software and dependencies for your model, such as Python, TensorFlow, or PyTorch.\\n2. Download or transfer your trained model to your on-premise environment.\\n3. Set up a server or cluster to run the model, either using existing infrastructure or by purchasing additional\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> 1. Limited scalability: On-premise infrastructure may have limited capacity to handle large volumes of data and computational workloads, which can limit the scalability of your models.\\r\\n\\r\\n2. Maintenance and upgrades: You are responsible for maintaining and upgrading the hardware and software infrastructure required\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n> 1. Cost: On-premise deployment can be more expensive than cloud-based deployment, as it requires hardware and infrastructure to be purchased and maintained.\\n2. Scalability: On-premise deployment may not be as scalable as cloud-based deployment, as it requires physical infrastructure to be added\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8447/v1\", max_tokens=128)\nmessages = [HumanMessage(content=\"Why do I need to run machine learning models on-premise?\")]\nprint(chat(messages))\n\n# output:\n```\n\nFor using it in a chat setting we recommend using a Chat Prompt Template as shown below:\n    \n```python\n\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat_template = \"\"\"\nYou are an AI assistant in a conversational setting.\nProvide a conversational answer to any question an User asks. Be original, concise, accurate and helpful.\n===================\n\nUser: {user_message}\nAssistant:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=chat_template,\n)\n\nuser_message = \"Why do I need to run machine learning models on-premise?\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8447/v1\", max_tokens=128)\nchain = LLMChain(llm=chat, prompt=prompt, verbose=True)\nprint(chain.run(user_message=user_message))\n\n```\n\n### \ud83d\udd0e Quality Benchmarks\n\n### \ud83d\udeab Limitations and Biases\n\nThe Mistral 7B Instruct model serves as a quick demonstration of how the base model can be readily fine-tuned to achieve compelling performance. It lacks any moderation mechanisms. The developers anticipate engaging with the community on methods to refine the model's adherence to guardrails, enabling its deployment in environments necessitating moderated outputs.\n\n## \ud83d\udcdc License\nIt is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions.", "serviceType": "binary", "version": "1", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/chat-mistral-7b-instruct/logo.svg", "modelInfo": {"memoryRequirements": 4800}, "interfaces": ["chat"], "defaultPort": 8000, "defaultExternalPort": 8447, "weightsDirectoryUrl": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/", "weightsFiles": ["mistral-7b-instruct-v0.1.Q5_0.gguf"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/premAI-io/prem-services/releases/download/v1/cht-llama-cpp-mistral-1-aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "cht-llama-cpp-mistral-1-aarch64-apple-darwin --model-path=mistral-7b-instruct-v0.1.Q5_0.gguf --port=8447", "banner": null}, {"id": "whisper-large-v2", "name": "Whisper Large v2", "description": "", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need for fine-tuning. <a href='https://huggingface.co/openai/whisper-large-v2' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `whisper-large-v2` service on Prem, you'll just need a GPU with at least 6GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n\nWhisper can be used for various tasks, including English to English transcription, French to French transcription, and French to English translation. It can also handle long-form transcription by using a chunking algorithm, allowing it to transcribe audio samples of arbitrary length.\n\n### \ud83c\udfb6 sample.wav. You can find the file [here](https://github.com/premAI-io/prem-registry/blob/main/audio-to-text-whisper-tiny/sample.wav)\n\n<img width=\"1449\" alt=\"image\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/8fe4ee74-e941-42ae-b3e8-5efd30581729\">\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with OpenAI Python client\n\nThe service exposes the same endpoints as OpenAI DALL-E does. You can directly use the official `openai` python library.\n\n```python\n\n!pip install openai\n\nimport openai\n\nopenai.api_base = \"http://184.105.5.51:10111/v1\"\nopenai.api_key = \"random-string\"\n\naudio_file = open(\"./sample.wav\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\nprint(transcript)\n\n```\n\n## \ud83d\udcdc License\n\nWhisper's code and model weights are released under the MIT License.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/audio-to-text-whisper-large-v2/logo.svg", "modelInfo": {"memoryRequirements": 6144}, "interfaces": ["audio-to-text"], "dockerImages": {"gpu": {"size": 23723729305, "image": "ghcr.io/premai-io/audio-to-text-whisper-large-v2-gpu:1.0.1"}}, "defaultPort": 8000, "defaultExternalPort": 10111, "banner": null}, {"id": "redis-vector-db", "name": "Redis Vector DB", "description": "Redis, short for Remote Dictionary Server, serves as a multifunctional in-memory data structure store. It functions as a distributed key-value database, cache, and message broker, all operating in-memory for high-speed data access. With optional durability, Redis ensures data persistence despite potential system failures. [Learn More](https://redis.com/solutions/use-cases/vector-database/)", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nRedis, short for Remote Dictionary Server, serves as a multifunctional in-memory data structure store. It functions as a distributed key-value database, cache, and message broker, all operating in-memory for high-speed data access. With optional durability, Redis ensures data persistence despite potential system failures. <a href='https://redis.com/solutions/use-cases/vector-database/' target='_blank'>Learn more</a> \ud83d\ude80.\n\n## \ud83d\udc47 Getting Started (Implementation)\n\nThe service can be used with Langchain. You can check the <a href='https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/redis.html' target='_blank'>official documentation</a>. In the code snippet below, weassume that you are using <a href='https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2' target='_blank'>`all-miniLM-l6-v2`</a> model for embeddings generation and the service is running locally on port 8001.\n\n```python\n\n!pip install redis\n\nimport os\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.docstore.document import Document\nfrom langchain.vectorstores.redis import Redis\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\ndoc1 = Document(page_content=\"Prem is an easy to use open source AI platform. With Prem you can quickly build provacy preserving AI applications.\")\ndoc2 = Document(page_content=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(page_content=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")\n\n# Using sentence transformers all-MiniLM-L6-v2\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/v1\")\n\n# Using locally running Redis\nurl = \"redis://localhost:6379\"\n\nvectorstore = Redis.from_documents([doc1, doc2, doc3], embeddings, redis_url=url,  index_name=\"prem_index_test\")\n\nquery = \"What are Prem Benefits?\"\ndocs = vectorstore.similarity_search(query)\nprint(docs[0].page_content)\n```", "interfaces": ["vector-store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/vector-store-redis/logo.svg", "modelInfo": {}, "volumePath": "/data", "dockerImages": {"cpu": {"size": 261514099, "image": "redis/redis-stack-server:latest"}}, "defaultPort": 6379, "defaultExternalPort": 6379, "banner": null}, {"id": "stable-diffusion-2", "name": "Stable Diffusion 2", "description": "Stable Diffusion v2 is an enhanced version of the Stable Diffusion v2-base model, developed by Robin Rombach and Patrick Esser. This model is designed to generate and modify images based on text prompts, utilizing a Latent Diffusion Model with a fixed, pretrained text encoder (OpenCLIP-ViT/H). The model was initially trained from the Stable Diffusion v2-base model and then further trained for an additional 150k steps using a v-objective on the same dataset. It was then resumed for another 140k steps on 768x768 images.", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nStable Diffusion v2 is an enhanced version of the Stable Diffusion v2-base model, developed by Robin Rombach and Patrick Esser. This model is designed to generate and modify images based on text prompts, utilizing a Latent Diffusion Model with a fixed, pretrained text encoder (OpenCLIP-ViT/H). The model was initially trained from the Stable Diffusion v2-base model and then further trained for an additional 150k steps using a v-objective on the same dataset. It was then resumed for another 140k steps on 768x768 images. <a href='https://stability.ai/blog/stable-diffusion-v2-release' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `stable-diffusion-2` service on Prem, you'll need access to a GPU with at least 16GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Iron man portrait, highly detailed, science fiction landscape, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart\n\n![k5h9_ilY](https://github.com/premAI-io/prem-registry/assets/29598954/49d162c9-a308-466c-a038-9bb54d2009fd)\n\n### 2\ufe0f\u20e3 Prompt: Low polygon panda 3d\n\n![hPCoZERY](https://github.com/premAI-io/prem-registry/assets/29598954/51537f29-f4cc-469f-88c4-ad18559cb043)\n\n### 3\ufe0f\u20e3 Prompt: 3d hiper-realistic rick sanchez and morty\n\n![NWVcWCfw](https://github.com/premAI-io/prem-registry/assets/29598954/667d08ad-7dd7-436f-8e2e-5e05d547653d)\n\n### 4\ufe0f\u20e3 Prompt: Synthwave brad pitt wearing headphones, animated, trending on artstation, portrait\n\n![q6yKHgDv](https://github.com/premAI-io/prem-registry/assets/29598954/9b88388d-08b7-4a9a-b9a3-766497b3403a)\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with OpenAI Python client\n\nThe service exposes the same `/image/generations` (for Text-to-Image) and `image/edits` (for Prompt+Image-to-Image) endpoints as OpenAI DALL-E does. You can directly use the official `openai` python library.\n\n#### Text-to-Image\n\n```python\n\n!pip install openai\n!pip install pillow\n\nimport io\nimport base64\nimport openai\n\nfrom PIL import Image\n\nopenai.api_base = \"http://localhost:9222/v1\"\nopenai.api_key = \"random-string\"\n\nresponse = openai.Image.create(\n    prompt=\"Iron man portrait, highly detailed, science fiction landscape, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart\",\n    n=1,\n    size=\"512x512\"\n)\n\nimage_string = response[\"data\"][0][\"b64_json\"]\n\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"iron_man.jpeg\")\n\n```\n\n#### Prompt + Image-to-Image\n\n```python\nimport io\nimport base64\nimport openai\n\nfrom PIL import Image\n\nopenai.api_base = \"http://localhost:9222/v1\"\nopenai.api_key = \"random-string\"\n\nresponse = openai.Image.create_edit(\n  image=open(\"astronaut.png\", \"rb\"), #assuming you have an astronaut floating image\n  prompt=\"astronaut floating in dark space, going down towards earth. Super high resolution, unreal engine, ultra realistic\",\n  n=1,\n  guidance_scale=9,\n  num_inference_steps=50,\n  size=\"512x512\",\n)\n\nimage_string = response[\"data\"][0][\"b64_json\"]\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"astronaut_edit.png\", \"PNG\")\n```\n\n## \ud83d\udcdc License\n\nThe model is under CreativeML Open RAIL++-M License.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/diffuser-stable-diffusion-2/logo.svg", "modelInfo": {"memoryRequirements": 14705, "secondsPerImage": 8}, "interfaces": ["diffuser"], "dockerImages": {"gpu": {"size": 26660796526, "image": "ghcr.io/premai-io/diffuser-stable-diffusion-2-gpu:1.0.3"}}, "defaultPort": 8000, "defaultExternalPort": 9222, "banner": null}, {"id": "replit-v1-3b", "name": "Replit v1 3B", "description": "", "documentation": "# Documentation", "comingSoon": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/coder-replit-v1-3b/logo.svg", "modelInfo": {}, "interfaces": ["coder"], "dockerImages": {"gpu": {"size": 27258256302, "image": "ghcr.io/premai-io/coder-replit-code-v1-3b-gpu:0.0.2"}}, "defaultPort": 8000, "defaultExternalPort": 10111, "banner": null}, {"id": "bark", "name": "Bark", "description": "Bark is a transformer-based text-to-audio model created by Suno https://suno.ai/. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise, and simple sound effects.", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nBark is a transformer-based text-to-audio model created by Suno https://suno.ai/. Bark can generate highly realistic, multilingual speech as well as other audio - including music, background noise, and simple sound effects. <a href='https://github.com/suno-ai/bark' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `bark` service on Prem, you'll just need a GPU or CPU with at least 6GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n\n### 2\ufe0f\u20e3 Prompt: Hello, my name is Suno. And, uh \u2014 and I like pizza. [laughs] But I also have other interests such as playing tic tac toe.\n\nYou can check the output [here](https://github.com/suno-ai/bark#-usage-in-python).\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with Python\n\n```python\n\nimport requests\n\nprompt = \"\"\"\nHello, my name is Suno. And, uh \u2014 and I like pizza. [laughs] \nBut I also have other interests such as playing tic tac toe.\n\"\"\"\n\nresponse = requests.post(\"http://localhost:10111/v1/audio/generation\",\n                         json={\"prompt\": prompt})\nresponse_content = requests.get(\n    f\"http://localhost:10111/files/{response.json()['url']}\")\n\nwith open(\"output_file.wav\", \"wb\") as f:\n    f.write(response_content.content)\n\n```\n\n## \ud83d\udcdc License\n\nBark was developed for research purposes. It is not a conventional text-to-speech model but instead a fully generative text-to-audio model, which can deviate in unexpected ways from provided prompts. Bark is licensed under the MIT License.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/text-to-audio-bark/logo.svg", "modelInfo": {"memoryRequirements": 7065}, "interfaces": ["text-to-audio"], "dockerImages": {"cpu": {"size": 18393188455, "image": "ghcr.io/premai-io/text-to-audio-bark-cpu:1.0.0"}, "gpu": {"size": 34699224198, "image": "ghcr.io/premai-io/text-to-audio-bark-gpu:1.0.0"}}, "defaultPort": 8000, "defaultExternalPort": 10111, "banner": null}, {"id": "mistral-7b-128k", "name": "Mistral 7B 128K", "beta": true, "description": "", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\nThe Mistral 7B Instruct is a model built by finetuning  <a href='https://huggingface.co/mistralai/Mistral-7B-v0.1' target='_blank'>Mistral-7B-v0.1</a>.\n\n## \ud83d\udcd2 Example Usage\n\nFollowing are a few example generations with `max_token=64` set.\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> 1. Security and Privacy: On-premise machine learning allows organizations to maintain full control over their data, ensuring that sensitive information is not compromised during transmission or storage. This can be particularly important for industries such as healthcare or finance where data privacy regulations are strict.\\n2. Customization: On-p\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> 1. Install the necessary software and dependencies for your model, such as Python, TensorFlow, or PyTorch.\\n2. Download or transfer your trained model to your on-premise environment.\\n3. Set up a server or cluster to run the model, either using existing infrastructure or by purchasing additional\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> 1. Limited scalability: On-premise infrastructure may have limited capacity to handle large volumes of data and computational workloads, which can limit the scalability of your models.\\r\\n\\r\\n2. Maintenance and upgrades: You are responsible for maintaining and upgrading the hardware and software infrastructure required\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n> 1. Cost: On-premise deployment can be more expensive than cloud-based deployment, as it requires hardware and infrastructure to be purchased and maintained.\\n2. Scalability: On-premise deployment may not be as scalable as cloud-based deployment, as it requires physical infrastructure to be added\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8447/v1\", max_tokens=128)\nmessages = [HumanMessage(content=\"Why do I need to run machine learning models on-premise?\")]\nprint(chat(messages))\n\n# output:\n```\n\nFor using it in a chat setting we recommend using a Chat Prompt Template as shown below:\n    \n```python\n\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat_template = \"\"\"\nYou are an AI assistant in a conversational setting.\nProvide a conversational answer to any question an User asks. Be original, concise, accurate and helpful.\n===================\n\nUser: {user_message}\nAssistant:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=chat_template,\n)\n\nuser_message = \"Why do I need to run machine learning models on-premise?\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8447/v1\", max_tokens=128)\nchain = LLMChain(llm=chat, prompt=prompt, verbose=True)\nprint(chain.run(user_message=user_message))\n\n```\n\n### \ud83d\udd0e Quality Benchmarks\n\n### \ud83d\udeab Limitations and Biases\n\nThe Mistral 7B Instruct model serves as a quick demonstration of how the base model can be readily fine-tuned to achieve compelling performance. It lacks any moderation mechanisms. The developers anticipate engaging with the community on methods to refine the model's adherence to guardrails, enabling its deployment in environments necessitating moderated outputs.\n\n## \ud83d\udcdc License\nIt is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions.", "serviceType": "binary", "version": "1", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/chat-mistral-7b-128k/logo.svg", "modelInfo": {"memoryRequirements": 4800}, "interfaces": ["chat"], "defaultPort": 8337, "defaultExternalPort": 8337, "weightsDirectoryUrl": "https://huggingface.co/TheBloke/Yarn-Mistral-7B-128k-GGUF/resolve/main/", "weightsFiles": ["yarn-mistral-7b-128k.Q4_K_M.gguf"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/premAI-io/prem-services/releases/download/v1/cht-llama-cpp-mistral-1-aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "cht-llama-cpp-mistral-1-aarch64-apple-darwin --model-path=yarn-mistral-7b-128k.Q4_K_M.gguf --port=8337 --ctx=128000", "banner": null}, {"id": "qdrant-vector-db-binary", "name": "Qdrant", "description": "Qdrant - High-performance, massive-scale Vector Database for the next generation of AI", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\n<a href='https://qdrant.tech/' target='_blank'>Qdrant</a> is a vector similarity search engine designed for storing, searching, and managing points along with their respective payloads. Built with an emphasis on extensive filtering, it is particularly beneficial for neural network matching, semantic-based matching, and faceted search. Qdrant offers various deployment options including local mode, on-premise server deployment, and Qdrant Cloud, each catering to different use-case scenarios. <a href='https://qdrant.tech/documentation' target='_blank'>Learn more</a> \ud83d\ude80.\n\n## \ud83d\udc47 Getting Started (Implementation)\n\nThe service can be used with Langchain or the official qdrant python client (https://github.com/qdrant/qdrant). Below you can find an example using the service with Langchain. In the code snippet, we are assuming that you are using all-miniLM-l6-v2 model for embeddings generation and the service is running locally on port 8001.\n\n```python\n\n!pip install qdrant-client\n\nimport os\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.docstore.document import Document\nfrom langchain.vectorstores import Qdrant\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\ndoc1 = Document(page_content=\"Prem is an easy to use open source AI platform. With Prem you can quickly build provacy preserving AI applications.\")\ndoc2 = Document(page_content=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(page_content=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")\n\n# Using sentence transformers all-MiniLM-L6-v2\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/v1\")\n\n# Using locally running Qdrant\nurl = \"http://localhost:6333\"\n\nvectorstore = Qdrant.from_documents(\n    [doc1, doc2, doc3], \n    embeddings, \n    url=url, \n    collection_name=\"prem_collection_test\",\n)\n\nquery = \"What are Prem Benefits?\"\ndocs = vectorstore.similarity_search(query)\nprint(docs[0].page_content)\n```", "version": "1", "serviceType": "binary", "beta": true, "interfaces": ["vector-store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/vector-store-qdrant-binary/logo.svg", "modelInfo": {}, "defaultPort": 6333, "defaultExternalPort": 6333, "weightsDirectoryUrl": "https://raw.githubusercontent.com/premAI-io/prem-registry/main/", "weightsFiles": ["keep.txt"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/tiero/qdrant/releases/download/v1/qdrant-aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "qdrant-aarch64-apple-darwin", "skipHealthCheck": true, "banner": null}, {"id": "all-minilm-l6-v2-binary", "name": "All MiniLM L6 v2", "description": "All-MiniLM-L6-v2 is a sentence-transformers model designed to map sentences and paragraphs to a 384-dimensional dense vector space, ideal for clustering or semantic search tasks. Developed during Hugging Face's Community week, this model is fine-tuned on a 1B sentence pairs dataset with a contrastive learning objective. It excels in encoding short texts, capturing semantic information, and is useful for information retrieval, clustering, or sentence similarity tasks. [Learn More](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nAll-MiniLM-L6-v2 is a sentence-transformers model designed to map sentences and paragraphs to a 384-dimensional dense vector space, ideal for clustering or semantic search tasks. Developed during <a href='https://huggingface.co/' target='_blank'>Hugging Face</a>'s Community week, this model is fine-tuned on a 1B sentence pairs dataset with a contrastive learning objective. It excels in encoding short texts, capturing semantic information, and is useful for information retrieval, clustering, or sentence similarity tasks. <a href='https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2' target='_blank'>Learn more</a> \ud83d\ude80.\n\n## \ud83d\udc47 Getting Started\n\nThe service is compatible with \ud83e\udd9c\ud83d\udd17<a href='https://github.com/hwchase17/langchain' target='_blank'>LangChain</a> and follows OpenAI <a href='https://platform.openai.com/docs/api-reference' target='_blank'>API request-response</a> format. If you haven't already, you will need to install :\n\n* `langchain` \u27a1\ufe0f <a href='https://pypi.org/project/langchain/' target='_blank'>pip install</a>.\n* `tiktoken` \u27a1\ufe0f <a href='https://pypi.org/project/tiktoken/' target='_blank'>pip install</a>.\n* `openai` \u27a1\ufe0f <a href='https://pypi.org/project/openai/' target='_blank'>pip install</a>.\n\n## \u2692\ufe0f Usage\n\n\ud83d\udc49 Find an example for using the service with \ud83e\udd9c\ud83d\udd17 LangChain below:\n\n```python\nimport os\n\nfrom langchain.embeddings import OpenAIEmbeddings\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/v1\")\ntext = \"Prem is an easy to use open source AI platform.\"\nquery_result = embeddings.embed_query(text)\ndoc_result = embeddings.embed_documents([text])\n```\n\n\nAlso check the official sentence transformers <a href='https://www.sbert.net/' target='_blank'>documentation</a>. It provides extensive examples and detailed information for using the model.\n\n## \ud83d\udc40 Intended Uses\nThe model is meant to be used as an encoder for single sentences and short paragraphs. Given an input text, it outputs a vector that captures the semantic information. You can use the sentence vector generated for information retrieval, clustering,\u00a0or sentence similarity tasks.\n\nBy default, input text longer than 256-word\u00a0pieces is truncated.\n\n## \ud83d\udd0e Evaluation Results\nFor an automated evaluation of this model, see the Sentence Embeddings Benchmark <a href='https://seb.sbert.net' target='_blank'>page</a>.\n\n## \u2696\ufe0f License\n\nThe model is published under <a href='https://www.apache.org/licenses/LICENSE-2.0' target='_blank'>Apache License 2.0</a>.\n", "serviceType": "binary", "version": "1", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/embeddings-all-minilm-l6-v2-binary/logo.svg", "modelInfo": {"memoryRequirements": 8192}, "interfaces": ["embeddings"], "defaultPort": 8444, "defaultExternalPort": 8444, "weightsDirectoryUrl": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/", "weightsFiles": ["1_Pooling/config.json", "config.json", "config_sentence_transformers.json", "data_config.json", "modules.json", "pytorch_model.bin", "sentence_bert_config.json", "special_tokens_map.json", "tokenizer.json", "tokenizer_config.json", "train_script.py", "vocab.txt"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/premAI-io/prem-services/releases/download/v1/ebd-all-minilm-1-aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "ebd-all-minilm-1-aarch64-apple-darwin --port=8444 --model-dir .", "banner": null}, {"id": "falcon-7b-instruct", "name": "Falcon 7B Instruct", "beta": true, "description": "Falcon 7B Instruct, developed by TII, is a 7B parameters causal decoder-only model trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. The model is particularly designed for commercial use and its inference can be run on various GPU configurations.", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nFalcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. The model is particularly designed for commercial use and its inference can be run on various GPU configurations. <a href='https://huggingface.co/tiiuae/falcon-7b-instruct' target='_blank'>Learn More</a>\n\n## \ud83d\udcbb Hardware Requirements\n\n> **Memory requirements**: 15.81 GB (15085 MiB).\n\nTo run the `falcon-7b-instruct` service, you'll need the following hardware configuration:\n\n### Cloud Platforms\n\nIf you are using AWS:\n\n- Instance Type: `p3.2xlarge` or higher\n- GPU: NVIDIA A100, NVIDIA V100\n\nIf you are using Paperspace:\n\n- Instance Type: `V100` or higher\n- GPU: NVIDIA A100, NVIDIA V100\n\n### On-Premise Platforms\n\nYou'll need access to a GPU with the following options:\n- A100 GPUs: A100 GPUs are preferred for training all model sizes.\n- V100 GPUs: The model can be run on V100 GPUs.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> \\nRunning machine learning models on-premise allows organizations to have more control over their data and computations. It also allows for the processing of large amounts of data in a high-performance environment, which can speed up the training and inference of models. Additionally, on-premise computing allows for integration with other applications and services.The main benefit of running a machine learning model on-premise is the ability to have more control over the data and computations, as well as the ability to process large amounts of data in a high-performance environment.\n\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> \\nTo run models on-premise, you will need to ensure that your data and models are properly stored and secured. This can be done through various means, such as storing your data on your own cloud storage solution or using a public cloud platform. Additionally, you will need to ensure that you have appropriate permissions and access to your data and models. It is recommended to consult with your IT department or a data modeling expert to determine the best approach for running your models on-premise.The main problem that many companies face is the lack of scalability in their current infrastructure.\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> \\nThere are several limitations of running your models on-premise, such as hardware and software limitations, performance issues, network latency, security issues, and licensing costs. Additionally, running models on-premise may require significant infrastructure resources and maintenance, which can be costly.Powered by TradeKingThe best way to test a trading strategy is to use the demo environment provided by the trading platform.I also need to know how to set up the trading platform on my computer in order to use it for trading.\\n- Can I use the trading platform on\n\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n<blockquote>\n\n \\nDeploying models on-premise can offer several benefits, including improved latency, security, and data privacy. However, there are also trade-offs to consider, such as increased complexity, cost, and scalability. Additionally, on-premise models may require more resources for scaling and managing data, which can increase costs over time. It is important to weigh the costs and benefits of on-premise vs. cloud-based deployment to determine the best fit for your specific needs.I hope this helps!-EI\\nWhat are some best practices for ensuring security when deploying models on-premise?\\nAs models are deployed on-premise, it is important to ensure security measures to protect sensitive data and infrastructure. Some best practices include encrypting data transmissions, using role-based access controls, and implementing firewalls and intrusion detection systems. Additionally, regular security audits and evaluations should be conducted to ensure continued security measures. -EIThis is a great resource on best practices for deploying models on-premise. -EIThe following article offers a more in-depth guide to securing your on-premise deployment: \n\n</blockquote>\n\nIt's visible above from the outputs that model sometimes generates gibberish at the end and tends to hallucinate. We recommend users of Falcon-7B-Instruct to develop guardrails and to take appropriate precautions for any production use as it's only a text generation model by default.\n\nAn example would be using a Chat Prompt Template as shown below:\n\nPrompt:\n```\nYou are an AI assistant in a conversational setting.\nProvide a concise and accurate conversational answer to anything User asks.\n===================\n\nUser: What are the trade-offs of deploying models on-premise I should be aware of?\nAssistant:\"\"\"\n```\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8448/v1\", max_tokens=128)\nmessages = [HumanMessage(content=\"What are the trade-offs of deploying models on-premise I should be aware of?\")]\nprint(chat(messages))\n```\n\nFor using it in a chat setting we recommend using a Chat Prompt Template as shown below:\n    \n```python\n\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat_template = \"\"\"\nYou are an AI assistant in a conversational setting.\nProvide a conversational answer to any question an User asks. Be original, concise, accurate and helpful.\n===================\n\nUser: {user_message}\nAssistant:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=chat_template,\n)\n\nuser_message = \"Why do I need to run machine learning models on-premise?\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8448/v1\", max_tokens=128)\nchain = LLMChain(llm=chat, prompt=prompt, verbose=True)\nprint(chain.run(user_message=user_message))\n```\n\n### \ud83d\udd0e Quality Benchmarks\n\nIt outperforms comparable open-source models (e.g., MPT-7B, StableLM, RedPajama etc.)\n\nBase model Falcon-7B is trained on English and French data only, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### \ud83d\udeab Limitations and Biases\n\nWe have noticed that the model sometimes generates responses that are not relevant and mostly gibberish like letters and numbers or just repeating the same words. E.g - xjskdafhnwne$. Also we found that it generates hashtag like words like #falcon7b, #falcon7binstruct etc, which seems to be a bias coming from the finetuning data as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\nWe recommend users of Falcon-7B-Instruct to develop guardrails and to take appropriate precautions for any production use.\n\n\n## \ud83d\udcdc License\nFalcon-7B was trained on 1,500B tokens of <a href='https://huggingface.co/datasets/tiiuae/falcon-refinedweb' target='_blank'>RefinedWeb</a>, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by <a href='https://arxiv.org/abs/2101.00027' target='_blank'>The Pile (Gao et al., 2020).</a>\nIt is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions.", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/chat-falcon-7b-instruct/logo.svg", "modelInfo": {"memoryRequirements": 15085, "tokensPerSecond": 24}, "interfaces": ["chat"], "dockerImages": {"gpu": {"size": 34178471850, "image": "ghcr.io/premai-io/chat-falcon-7b-instruct-gpu:1.0.0"}}, "defaultPort": 8000, "defaultExternalPort": 8448, "banner": null}, {"id": "tabby-codellama-7B", "serviceType": "binary", "version": "1", "name": "Tabby CodeLlama 7B", "description": "", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nCode Llama is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 34 billion parameters. This is the repository for the base 7B version in the Hugging Face Transformers format. This model is designed for general code synthesis and understanding. Links to other models can be found in the index at the bottom.\n\n## \ud83d\udcbb Hardware Requirements\n\n\n## \ud83d\udcd2 Example Usage\n\n<img width=\"463\" alt=\"Screenshot 2023-08-01 at 22 46 39\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/04188ee4-9fbc-4d2d-9c3a-2aca359a92e3\">\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\udc2f Getting Started with Tabby VSCode Extension\n\n1. Install Tabby VSCode Extension <a href='https://marketplace.visualstudio.com/items?itemName=TabbyML.vscode-tabby' target='_blank'>here</a>\n\n<img width=\"1207\" alt=\"Screenshot 2023-08-01 at 12 27 29\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/05a85487-b2ad-4bcb-89fb-d610398b2f72\">\n\n2. Enable the extension and provide the URL where the service is running\n\n<img width=\"1180\" alt=\"Screenshot 2023-08-01 at 12 28 07\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/258eaa26-78cd-41a8-a4f1-c4924f0696aa\">\n\n3. Wait a few minutes\n\n> The model will be downloaded when the server starts. For this reason, you will need a few minutes before the extension starts to work properly.\n\n### \ud83d\udd0e Quality Benchmarks\n\nThe model has been evaluated on two code generation benchmarks: HumanEval and MTPB. Please refer to the <a href='https://arxiv.org/abs/2203.13474' target='_blank'>paper</a> for more details.\n\n### \ud83d\udeab Limitations and Biases\n\n\n## \ud83d\udcdc License\n\nThe model is under the 3-Clause BSD license.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/coder-tabby-codellama-7b/logo.svg", "modelInfo": {"memoryRequirements": 3204}, "interfaces": ["coder"], "defaultPort": 8080, "defaultExternalPort": 8467, "weightsDirectoryUrl": "https://huggingface.co/TabbyML/CodeLlama-7B/resolve/main/", "weightsFiles": ["tabby.json", "tokenizer.json", "ggml/q8_0.gguf"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/TabbyML/tabby/releases/download/v0.3.0/tabby_aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "tabby_aarch64-apple-darwin serve --model . --device metal --port 8467", "banner": null}, {"id": "qdrant", "name": "Qdrant", "description": "Qdrant is a vector similarity search engine designed for storing, searching, and managing points along with their respective payloads. Built with an emphasis on extensive filtering, it is particularly beneficial for neural network matching, semantic-based matching, and faceted search. Qdrant offers various deployment options including local mode, on-premise server deployment, and Qdrant Cloud, each catering to different use-case scenarios. [Learn More](https://qdrant.tech/documentation/)", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\n<a href='https://qdrant.tech/' target='_blank'>Qdrant</a> is a vector similarity search engine designed for storing, searching, and managing points along with their respective payloads. Built with an emphasis on extensive filtering, it is particularly beneficial for neural network matching, semantic-based matching, and faceted search. Qdrant offers various deployment options including local mode, on-premise server deployment, and Qdrant Cloud, each catering to different use-case scenarios. <a href='https://qdrant.tech/documentation' target='_blank'>Learn more</a> \ud83d\ude80.\n\n## \ud83d\udc47 Getting Started (Implementation)\n\nThe service can be used with Langchain or the official qdrant python client (https://github.com/qdrant/qdrant). Below you can find an example using the service with Langchain. In the code snippet, we are assuming that you are using all-miniLM-l6-v2 model for embeddings generation and the service is running locally on port 8001.\n\n```python\n\n!pip install qdrant-client\n\nimport os\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.docstore.document import Document\nfrom langchain.vectorstores import Qdrant\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\ndoc1 = Document(page_content=\"Prem is an easy to use open source AI platform. With Prem you can quickly build provacy preserving AI applications.\")\ndoc2 = Document(page_content=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(page_content=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")\n\n# Using sentence transformers all-MiniLM-L6-v2\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/v1\")\n\n# Using locally running Qdrant\nurl = \"http://localhost:6333\"\n\nvectorstore = Qdrant.from_documents(\n    [doc1, doc2, doc3], \n    embeddings, \n    url=url, \n    collection_name=\"prem_collection_test\",\n)\n\nquery = \"What are Prem Benefits?\"\ndocs = vectorstore.similarity_search(query)\nprint(docs[0].page_content)\n```", "interfaces": ["vector-store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/vector-store-qdrant/logo.svg", "modelInfo": {}, "volumePath": "/qdrant/storage", "dockerImages": {"cpu": {"size": 126913893, "image": "qdrant/qdrant:v1.0.3"}}, "defaultPort": 6333, "defaultExternalPort": 6333, "banner": null}, {"id": "stable-diffusion-xl-with-refiner", "name": "Stable Diffusion XL With Refiner", "description": "Stable Diffusion XL is a sophisticated text-to-image diffusion model capable of generating high-quality images from textual prompts. It also uses Stable Diffusion XL Refiner model to make the picture quality more refined.", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nStable Diffusion XL is a Diffusion-based text-to-image generative model by Stability AI. It is the base model is used to generate (noisy) latents, which can be further processed with a refinement model specialized for the final denoising steps. This service uses both base model and refinement model together. <a href='https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `stable-diffusion-xl-with-refiner` service on Prem, you'll need access to a GPU with at least 24GiB of VRAM.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Iron man portrait, highly detailed, science fiction landscape, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart\n![iron_man_refiner](https://github.com/premAI-io/prem-registry/assets/35634788/c9877a23-8999-4a71-883a-ce0444f06001)\n\n\n### 2\ufe0f\u20e3 Prompt: Low polygon panda 3d\n![panda_refiner](https://github.com/premAI-io/prem-registry/assets/35634788/db09d6e4-fe81-4d52-b6aa-4dda9f3a135d)\n\n\n### 3\ufe0f\u20e3 Prompt: 3d hiper-realistic rick sanchez and morty\n![rick_refiner](https://github.com/premAI-io/prem-registry/assets/35634788/f9855ccf-efd5-48a1-9b76-d248b4c73623)\n\n\n\n### 4\ufe0f\u20e3 Prompt: Synthwave brad pitt wearing headphones, animated, trending on artstation, portrait\n\n![brad_pitt_refiner](https://github.com/premAI-io/prem-registry/assets/35634788/65ff7a7a-1293-4fb5-997c-db08f3ea0eb1)\n\n\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with OpenAI Python client\n\nThe service exposes the same `/image/generations` (for Text-to-Image) endpoints as OpenAI DALL-E does. You can directly use the official `openai` python library.\n\n#### Text-to-Image\n\n```python\n\n!pip install openai\n!pip install pillow\n\nimport io\nimport base64\nimport openai\n\nfrom PIL import Image\n\nopenai.api_base = \"http://localhost:9225/v1\"\nopenai.api_key = \"random-string\"\n\nresponse = openai.Image.create(\n    prompt=\"Iron man portrait, highly detailed, science fiction landscape, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart\",\n    n=1,\n    size=\"512x512\"\n)\n\nimage_string = response[\"data\"][0][\"b64_json\"]\n\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"iron_man.png\", \"PNG\")\n\n```\n\n> **Note: Currently we only support text-to-image generation.**\n\n\n## \ud83d\udcdc License\n\nThe model is under CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/diffuser-stable-diffusion-xl-with-refiner/logo.svg", "modelInfo": {"memoryRequirements": 21662, "secondsPerImage": 7}, "interfaces": ["diffuser"], "dockerImages": {"gpu": {"size": 47738283226, "image": "ghcr.io/premai-io/diffuser-stable-diffusion-xl-with-refiner-gpu:1.0.3"}}, "defaultPort": 8000, "defaultExternalPort": 9225, "banner": null}, {"id": "llama-2-13b-chat", "name": "Llama v2 13B Chat", "beta": true, "description": "Llama v2 13B Chat, developed by Meta, is an auto-regressive language model that uses an optimized transformer architecture. It's a 13B parameters finetuned Chat model version using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. It's released under a custom commercial license.", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nLlama V2 13B Chat, developed by Meta, is an auto-regressive language model that uses an optimized transformer architecture. It's a 13B parameters finetuned Chat model version using supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. It's released under a custom commercial license and its inference can be run on various GPU configurations. <a href='https://huggingface.co/meta-llama/Llama-2-13b-chat' target='_blank'>Learn More</a>\n\n## \ud83d\udcbb Hardware Requirements\n\n> **Memory requirements**: 27.69 GB (26406 MiB).\n\nTo run the `llama-2-13b-chat` service, you'll need the following hardware configuration:\n\n### Cloud Platforms\n\nIf you are using AWS:\n\n- Instance Type: `p3.2xlarge` or higher\n- GPU: NVIDIA A100, NVIDIA V100 (2x)\n\nIf you are using Paperspace:\n\n- Instance Type: `V100-32G` or `A100` or higher\n- GPU: NVIDIA A100, NVIDIA V100\n\n### On-Premise Platforms\n\nYou'll need access to a GPU with the following options:\n- A100 GPUs: A100 GPUs are preferred for training all model sizes, and are the only GPUs that can train the 13B param model in a reasonable amount of time.\n- A10 GPUs: Training the 13B param model is not recommended on A10s.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> \\n\\nThere are several reasons why you might want to run machine learning models on-premise, rather than in the cloud:\\n\\n1. Data privacy and security: If you are working with sensitive data, you may not want to transmit it to the cloud for processing. By running your models on-premise, you can keep your data within your own network and maintain greater control over its security.\\n2. Performance: Depending on the size and complexity of your data, you may find that running your models on-premise is\n\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> \\n\\nYou can run your machine learning models on-premise using a variety of methods, depending on your specific needs and requirements. Here are some general strategies you can consider:\\n\\n1. Install the necessary software and libraries: You will need to install the necessary software and libraries for your machine learning models on your on-premise environment. This may include popular machine learning frameworks like scikit-learn, TensorFlow, or PyTorch, as well as other dependencies like Python or R.\\n2. Use a containerization platform: Containerization\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> \\n\\nThe limitations of running your machine learning models on-premise include:\\n\\n1. Compute and storage constraints: You may have limited compute and storage resources on-premise, which can limit the size and complexity of the models you can train.\\n2. Maintenance and upgrades: On-premise infrastructure requires regular maintenance and upgrades, which can be time-consuming and may disrupt your model training and deployment.\\n3. Security and compliance: On-premise infrastructure may\n\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n<blockquote>\n\n \\n\\nThere are several trade-offs to consider when deploying machine learning models on-premise, including:\\n\\n1. Cost: On-premise deployment can be more expensive than cloud-based deployment, as it requires investment in hardware, software, and maintenance.\\n2. Security: On-premise deployment can provide better security and control over data, but it also requires additional security measures to protect against unauthorized access and data breaches.\\n3. Scalability: On\n\n</blockquote>\n\nWe recommend users of Llama-V2-13B-Chat to develop guardrails and to take appropriate precautions for any production use as it's only a text generation model by default, tuned to give responses in a QnA setting.\n\nAn example would be using a Chat Prompt Template as shown below:\n\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8230/v1\", max_tokens=4096)\nmessages = [HumanMessage(content=\"What are the trade-offs of deploying models on-premise I should be aware of?\")]\nprint(chat(messages))\n```\n\n> To know more on how to handle multi-turn conversation prompts specially for Llama-v2, check out: https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n\n\n### \ud83d\udeab Limitations and Biases\n\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2\u2019s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide released by <a href='https://ai.meta.com/llama/responsible-use-guide/' target='_blank'>Meta here.</a>\n\n## \ud83d\udcdc License\nIt is made available under a custom commercial license, which is available <a href='https://ai.meta.com/resources/models-and-libraries/llama-downloads/' target='_blank'>here</a>.", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/chat-llama-2-13b-chat/logo.svg", "modelInfo": {"memoryRequirements": 26406, "tokensPerSecond": 29}, "interfaces": ["chat"], "dockerImages": {"gpu": {"size": 45859419975, "image": "ghcr.io/premai-io/chat-llama-2-13b-chat-gpu:1.0.1"}}, "defaultPort": 8000, "defaultExternalPort": 8230, "banner": null}, {"id": "pgvector", "name": "PGVector", "description": "It is a combination of PostgreSQL, an open-source relational database, and PGVector, an extension that enables vector-based operations and similarity searches. PGVector allows you to store and query vector embeddings efficiently within the PostgreSQL database.", "documentation": "\n# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\n[PGVector](https://github.com/pgvector/pgvector) is an integration of PostgreSQL with vector-based operations and similarity searches. It allows you to store and query vector embeddings efficiently within the PostgreSQL database. PGVector is a powerful tool for building AI and ML applications that require similarity search capabilities. It supports:\n- exact and approximate nearest neighbor search\n- L2 distance, inner product, and cosine distance\n\nLearn more about <a href='https://github.com/pgvector/pgvector' target='_blank'>PGVector here</a> \ud83d\ude80.\n\n## \ud83d\udc47 Getting Started (Implementation)\n\nThe service can be used with Langchain. You can check the [langchain pgvector documentation](https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/pgvector) for detailed usage instructions. In the code snippet below, we assume that you have installed the required dependencies present in *Pre-requisites* step and are using `all-miniLM-l6-v2` model for embeddings generation and the service is running locally on port `8444`.\n\n\n### Pre-requisites\n\n```bash\npip install pgvector tiktoken psycopg2-binary openai langchain\n```\n\n```python\n\nimport os\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.docstore.document import Document\nfrom langchain.vectorstores.pgvector import PGVector\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\ndoc1 = Document(page_content=\"Prem is an easy to use open source AI platform. With Prem you can quickly build provacy preserving AI applications.\")\ndoc2 = Document(page_content=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(page_content=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")\n\n# Using sentence transformers all-MiniLM-L6-v2\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/v1\")\n\n# Using locally running PostgreSQL connection\nCONNECTION_STRING = PGVector.connection_string_from_db_params(\n    driver=\"psycopg2\",\n    host=\"localhost\",\n    port=\"5432\",\n    database=\"postgres\",\n    user=\"postgres\",\n    password=\"postgres\",\n)\n\n# Create PGVector vectorstore instance\nvectorstore = PGVector.from_documents(\n    documents=[doc1, doc2, doc3],\n    embedding=embeddings,\n    connection_string=CONNECTION_STRING,\n)\n\n# Perform similarity search\nquery = \"What are Prem Benefits?\"\ndocs = vectorstore.similarity_search(query)\nprint(docs[0].page_content)\n```\n", "interfaces": ["vector-store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/vector-store-pgvector/logo.svg", "modelInfo": {}, "envVariables": ["POSTGRES_HOST=localhost", "POSTGRES_PORT=5432", "POSTGRES_DATABASE=postgres", "POSTGRES_USER=postgres", "POSTGRES_PASSWORD=postgres"], "execCommands": ["psql -U postgres -d postgres -c 'CREATE EXTENSION vector;'"], "volumePath": "/var/lib/postgresql/data", "dockerImages": {"cpu": {"size": 385485843, "image": "ankane/pgvector:v0.4.4"}}, "defaultPort": 5432, "defaultExternalPort": 5432, "banner": null}, {"id": "stable-diffusion-xl", "name": "Stable Diffusion XL", "description": "Stable Diffusion XL is a sophisticated text-to-image diffusion model capable of generating high-quality images from textual prompts.", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nStable Diffusion XL is a Diffusion-based text-to-image generative model by Stability AI. It is the base model is used to generate (noisy) latents, which can be further processed with a refinement model specialized for the final denoising steps. This service uses the base model only to generate image from text. <a href='https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `stable-diffusion-xl` service on Prem, you'll need access to a GPU with at least 16GiB of VRAM.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Iron man portrait, highly detailed, science fiction landscape, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart\n![iron_man](https://github.com/premAI-io/prem-registry/assets/35634788/755f6d9e-b13d-4e1a-bc9d-571e1b6e2490)\n\n\n### 2\ufe0f\u20e3 Prompt: Low polygon panda 3d\n![panda](https://github.com/premAI-io/prem-registry/assets/35634788/a8ef471b-dbb4-4dde-869d-80fa48d4bd48)\n\n\n### 3\ufe0f\u20e3 Prompt: 3d hiper-realistic rick sanchez and morty\n![rick](https://github.com/premAI-io/prem-registry/assets/35634788/b2abbf8d-39e7-4337-8e30-71d3701c4295)\n\n\n\n### 4\ufe0f\u20e3 Prompt: Synthwave brad pitt wearing headphones, animated, trending on artstation, portrait\n![brad_pitt](https://github.com/premAI-io/prem-registry/assets/35634788/d6f80543-9729-4090-82d6-4fea1d9ea608)\n\n\n\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with OpenAI Python client\n\nThe service exposes the same `/image/generations` (for Text-to-Image) endpoints as OpenAI DALL-E does. You can directly use the official `openai` python library.\n\n#### Text-to-Image\n\n```python\n\n!pip install openai\n!pip install pillow\n\nimport io\nimport base64\nimport openai\n\nfrom PIL import Image\n\nopenai.api_base = \"http://localhost:9224/v1\"\nopenai.api_key = \"random-string\"\n\nresponse = openai.Image.create(\n    prompt=\"Iron man portrait, highly detailed, science fiction landscape, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart\",\n    n=1,\n    size=\"512x512\"\n)\n\nimage_string = response[\"data\"][0][\"b64_json\"]\n\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"iron_man.png\", \"PNG\")\n\n```\n\n> **Note: Currently we only support text-to-image generation.**\n\n## \ud83d\udcdc License\n\nThe model is under CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/diffuser-stable-diffusion-xl/logo.svg", "modelInfo": {"memoryRequirements": 15806, "secondsPerImage": 4}, "interfaces": ["diffuser"], "dockerImages": {"gpu": {"size": 35585082235, "image": "ghcr.io/premai-io/diffuser-stable-diffusion-xl-gpu:1.0.3"}}, "defaultPort": 8000, "defaultExternalPort": 9224, "banner": null}, {"id": "stable-diffusion-2-1", "name": "Stable Diffusion 2.1", "description": "Stable Diffusion v2-1 is an advanced version of the Stable Diffusion v2 model, developed by Robin Rombach and Patrick Esser. This model is designed to generate and modify images based on text prompts, utilizing a Latent Diffusion Model with a fixed, pretrained text encoder (OpenCLIP-ViT/H). The model was initially fine-tuned from the Stable Diffusion v2 model and then further trained for an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nStable Diffusion v2-1 is an advanced version of the Stable Diffusion v2 model, developed by Robin Rombach and Patrick Esser. This model is designed to generate and modify images based on text prompts, utilizing a Latent Diffusion Model with a fixed, pretrained text encoder (OpenCLIP-ViT/H). The model was initially fine-tuned from the Stable Diffusion v2 model and then further trained for an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98. <a href='https://stability.ai/blog/stablediffusion2-1-release7-dec-2022' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `stable-diffusion-2-1` service on Prem, you'll need access to a GPU with at least 16GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Iron man portrait, highly detailed, science fiction landscape, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart\n\n![Xc3tlK4h](https://github.com/premAI-io/prem-registry/assets/29598954/3310b52f-aaeb-44fc-9bfa-9244ef6c0c6e)\n\n### 2\ufe0f\u20e3 Prompt: Low polygon panda 3d\n![E5MREIGA](https://github.com/premAI-io/prem-registry/assets/29598954/a11d02eb-90cc-4b3c-a7a4-8c8abc988bc6)\n\n### 3\ufe0f\u20e3 Prompt: 3d hiper-realistic rick sanchez and morty\n![zmicPjRq](https://github.com/premAI-io/prem-registry/assets/29598954/8ea64522-b255-452c-a06c-5050dfb65be6)\n\n### 4\ufe0f\u20e3 Prompt: Synthwave brad pitt wearing headphones, animated, trending on artstation, portrait\n\n![pXBajmuD](https://github.com/premAI-io/prem-registry/assets/29598954/4ddb1602-bf99-4921-aeb2-f1149556e476)\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with OpenAI Python client\n\nThe service exposes the same `/image/generations` (for Text-to-Image) and `image/edits` (for Prompt+Image-to-Image) endpoints as OpenAI DALL-E does. You can directly use the official `openai` python library.\n\n#### Text-to-Image\n\n```python\n\n!pip install openai\n!pip install pillow\n\nimport io\nimport base64\nimport openai\n\nfrom PIL import Image\n\nopenai.api_base = \"http://localhost:9111/v1\"\nopenai.api_key = \"random-string\"\n\nresponse = openai.Image.create(\n    prompt=\"Iron man portrait, highly detailed, science fiction landscape, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart\",\n    n=1,\n    size=\"512x512\"\n)\n\nimage_string = response[\"data\"][0][\"b64_json\"]\n\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"iron_man.jpeg\")\n\n```\n\n#### Prompt + Image-to-Image\n\n```python\nimport io\nimport base64\nimport openai\n\nfrom PIL import Image\n\nopenai.api_base = \"http://localhost:8000/v1\"\nopenai.api_key = \"random-string\"\n\nresponse = openai.Image.create_edit(\n  image=open(\"astronaut.png\", \"rb\"), #assuming you have an astronaut floating image\n  prompt=\"astronaut floating in dark space, going down towards earth. Super high resolution, unreal engine, ultra realistic\",\n  n=1,\n  guidance_scale=9,\n  num_inference_steps=50,\n  size=\"512x512\",\n)\n\nimage_string = response[\"data\"][0][\"b64_json\"]\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"astronaut_edit.png\", \"PNG\")\n```\n\n## \ud83d\udcdc License\n\nThe model is under CreativeML Open RAIL++-M License.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/diffuser-stable-diffusion-2-1/logo.svg", "modelInfo": {"memoryRequirements": 14705, "secondsPerImage": 8}, "interfaces": ["diffuser"], "dockerImages": {"gpu": {"size": 26660796556, "image": "ghcr.io/premai-io/diffuser-stable-diffusion-2-1-gpu:1.0.3"}}, "defaultPort": 8000, "defaultExternalPort": 9111, "banner": null}, {"id": "dalle-mini", "name": "DALL-E Mini", "description": "DALL-E Mini is a model that can be used to generate images based on text prompts. OpenAI had the first impressive model for generating images with DALL-E. DALL-E mini is an attempt at reproducing those results with an open-source model. It's released under Apache 2.0 License, allowing commercial usage", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nDall-E Mini is a text-to-image model attempt at reproducing OpenAI's DALL-E model results with an Open Source model. The model is trained by looking at millions of images from the internet with their associated captions. Over time, it learns how to draw an image from a text prompt.\nSome concepts are learned from memory as they may have seen similar images. However, it can also learn how to create unique images that don't exist, such as \"the Eiffel tower is landing on the moon,\" by combining multiple concepts together. A project report on the same by the creators can be found <a href='https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-mini-Generate-images-from-any-text-prompt--VmlldzoyMDE4NDAy' target='_blank'>here</a>\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `dalle-mini` service on Prem, you'll need access to a GPU with at least 32GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: hot tonkotsu ramen bowl, top view\n![hot_tonkotsu_ramen_bowl_top_view](https://github.com/premAI-io/prem-registry/assets/35634788/d89b8a7b-354b-474d-8df1-67475f9311cf)\n\n### 2\ufe0f\u20e3 Prompt: an illustration of doggo wearing a spacesuit\n![an_illustration_of_doggo_wearing_a_spacesuit](https://github.com/premAI-io/prem-registry/assets/35634788/bbb44e88-ceef-4fce-8017-f727bbfdc874)\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with OpenAI Python client\n\nThe service exposes the same endpoints as OpenAI DALL-E does. You can directly use the official `openai` python library.\n\n```python\n\n!pip install openai\n!pip install pillow\n\nimport io\nimport base64\nimport openai\n\nfrom PIL import Image\n\nopenai.api_base = \"http://localhost:8600/v1\"\nopenai.api_key = \"random-string\"\n\nresponse = response = openai.Image.create(\n    prompt=\"an illustration of doggo wearing a spacesuit\",\n    n=1\n)\n\nimage_string = response[\"data\"][0][\"b64_json\"]\n\nimg = Image.open(io.BytesIO(base64.decodebytes(bytes(image_string, \"utf-8\"))))\nimg.save(\"space_doggo.png\", format=\"PNG\")\n\n```\n\n## \ud83d\udcdc License\nIt's released under Apache 2.0 License, which enables commercial usage.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/diffuser-dalle-mini/logo.svg", "modelInfo": {"memoryRequirements": 37222, "secondsPerImage": 1.9}, "interfaces": ["diffuser"], "dockerImages": {"gpu": {"size": 11980514559, "image": "ghcr.io/premai-io/diffuser-dalle-mini-gpu:1.0.0"}}, "defaultPort": 8000, "defaultExternalPort": 8600, "banner": null}, {"id": "gorilla-falcon-7b", "name": "Gorilla Falcon 7B", "beta": true, "description": "Gorilla Falcon 7B is an open-source API caller trained by fine-tuning Falcon weights. The model is particularly designed for commercial use (License: Apache-2.0)", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\nGorilla Falcon 7B is an open-source API caller trained by fine-tuning Falcon weights. Given a natural language query, Gorilla can write a semantically- and syntactically- correct API to invoke. \n\n\n\n## \ud83d\udcbb Hardware Requirements\n\n**Memory requirements**: 10.6 GB (10874 MiB).\n\n\nTo run the `gorilla-falcon-7b` service, you'll need the following hardware configuration:\n\n### Cloud Platforms\n\nIf you are using AWS:\n\n- Instance Type: `p3.2xlarge` or higher\n- GPU: NVIDIA V100 or higher.\n\nIf you are using Paperspace:\n\n- Instance Type: `V100-32G` or higher\n- GPU: NVIDIA V100 or higher.\n\n### On-Premise Platforms\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Translate a text from English to French\n\n\\<\\<\\<domain\\>\\>\\>: Natural Language Processing Text2Text Generation\n\n\\<\\<\\<api_call\\>\\>\\>: T5ForConditionalGeneration.from_pretrained('google/byt5-small')\n\n\\<\\<\\<api_provider\\>\\>\\>: Hugging Face Transformers\n\n1. Import the necessary classes from the transformers package, which includes T5Tokenizer and T5ForConditionalGeneration.\n2. Load the pre-trained model 'google/byt5-small' using T5ForConditionalGeneration.from_pretrained() method. This model is specifically designed for text-to-text generation tasks.\n3. Load the tokenizer associated with the model using T5Tokenizer.from_pretrained('google/byt5-small') method.\n4. Prepare the input text by adding the prefix 'translate English to French:'before the text you want to translate.\n5. Tokenize the input text using the tokenizer and generate the output text using the model.\n6. Decode the output tokens to obtain the translated French text.\n\n### 2\ufe0f\u20e3 Prompt: Generate an image from a prompt\n\n\\<\\<\\<domain\\>\\>\\>: Multimodal Text-to-Image\n\n\\<\\<\\<api_call\\>\\>\\>: StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4')\n\n\\<\\<\\<api_provider\\>\\>\\>: Hugging Face\n1. Import the StableDiffusionPipeline class from the diffusers package and Image class from PIL.\n2. Load the pretrained StableDiffusionPipeline model with the identifier 'CompVis/stable-diffusion-v1-4'.\n3. Provide the text prompt describing the image you want to generate. For example, 'a beautiful landscape with a mountain range and a river'.\n4. Use the pipeline to generate an image based on the provided prompt. The generated image will be stored in the variable 'image'. You can then save the image to a file if needed.\n\n### 3\ufe0f\u20e3 Prompt: Upscale an image\n\n\\<\\<\\<domain\\>\\>\\>: Computer Vision Image-to-Image\n\n\\<\\<\\<api_call\\>\\>\\>: pipeline('image-super-resolution', model='caidas/swin2SR-classical-sr-x4-64')\n\n\\<\\<\\<api_provider\\>\\>\\>: Hugging Face Transformers\n1. Import the necessary libraries, which include the 'pipeline' function from transformers and the 'Image' class from PIL.\n2. Use the 'pipeline' function to create an instance of the 'image-super-resolution' model with the pre-trained 'caidas/ swin2SR-classical-sr-x4-64' model. This model is designed to upscale images by a factor of 4.\n3. Load the input image into the 'Image' class and pass it to the model for upscaling.\n4. The upscaled image will be saved in the same file with the prefix 'upscaled_'.\n\n\n### 4\ufe0f\u20e3 Prompt: Our customer is a zoo and we want to help them detect movement of different animals. Write a python program in 1 to 2 lines to call API in TensorFlowHub.\n\n```python\n# Import required libraries 'AutoImageProcessor' and 'PredictionLayer' from transformers, 'Image' from PIL, and'requests'. Load the animal detection image from a URL.\nprocessor = AutoImageProcessor.from_pretrained('hf-tiny-model-private/tiny-random-CLIPSegModel')\nmodel = PredictionLayer.from_pretrained('hf-tiny-model-private/tiny-random-CLIPSegModel')\nimage_url = 'https://example.com/animal_image.jpg'  # Replace this with the URL of the image you want to analyze.\nimage = Image.open(requests.get(image_url, stream=True).raw)  # Read the image from the URL and convert it to PIL Image format.\ninputs = processor(images=image, return_tensors='pt')  # Prepare inputs for the model by processing the image and converting it to the required format.\noutputs = model(**inputs)  # Apply the model to the inputs and obtain the output.\n# Analyze output for animal detections and take appropriate action.\n```\n\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8000/v1\")\nmessages = [HumanMessage(content=\"Generate an image from a text\")]\nprint(chat(messages))\n\n# output:\n# <<<domain>>>: Multimodal Text-to-Image Generation\n# <<<api_call>>>: StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\n# <<<api_provider>>>: Hugging Face\n# <<<explanation>>>:1. Import the necessary libraries: AutoencoderKL from diffusers.models and StableDiffusionPipeline from diffusers.\n# 2. Load the 'CompVis/stable-diffusion-v1-4' model and the'stabilityai/sd-vae-ft-mse' VAE model. The VAE model will be used for text encoding.\n# 3. Create a StableDiffusionPipeline instance by calling the from_pretrained method with the model and VAE as arguments.\n# 4. Provide a text prompt describing the desired image, and use the pipeline to generate an image based on the text prompt. Save the generated image to a file.\n```\n\nFor using it in a chat setting we recommend using a Chat Prompt Template as shown below:\n    \n```python\n\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat_template = \"\"\"\nUser: {user_message}\nAssistant: \n\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=chat_template,\n)\n\nuser_message = \"Generate an image from a text\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8000/v1\")\nchain = LLMChain(llm=chat, prompt=prompt, verbose=True)\nprint(chain.run(user_message=user_message))\n\n# output: Finished chain.\n# <<<domain>>>: Multimodal Text-to-Image Generation\n# <<<api_call>>>: StableDiffusionPipeline.from_pretrained('CompVis/stable-diffusion-v1-4', vae='AutoencoderKL.from_pretrained(stabilityai/sd-vae-ft-mse)')\n# <<<api_provider>>>: Hugging Face\n# <<<explanation>>>:1. Import the necessary libraries: AutoencoderKL from diffusers.models and StableDiffusionPipeline from diffusers.\n# 2. Load the 'CompVis/stable-diffusion-v1-4' model and the'stabilityai/sd-vae-ft-mse' VAE model. The VAE model will be used for text encoding.\n# 3. Create a StableDiffusionPipeline instance by calling the from_pretrained method with the model and VAE as arguments.\n# 4. Provide a text prompt describing the desired image, and use the pipeline to generate an image based on the text prompt. Save the generated image to a file.\n\n```\n\n### \ud83d\udeab Limitations and Biases\n\nWe have noticed that the model sometimes generates responses with reference to some random unexisting model name on Huggingface. Furthermore also the structure  of the output is not always the same (refer to above prompt examples). \nWe recommend users of Gorilla models to develop guardrails and to take appropriate precautions for any production use.\n\nThe creators of Gorilla Falcon 7B have mentioned that despite their effort in addressing the risks of hallucinations like other LLMs, Gorilla models are not free from such limitations. We hope our open-sourced codebase will help other researchers better understand these challenges and improve on these key limitations for making AI beneficial for everyone.\n\n\n## \ud83d\udcdc License\nIt is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions.", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/chat-gorilla-falcon-7b/logo.svg", "modelInfo": {"memoryRequirements": 10874, "tokensPerSecond": 22}, "interfaces": ["chat"], "dockerImages": {"gpu": {"size": 31291459579, "image": "ghcr.io/premai-io/chat-gorilla-falcon-7b-gpu:1.0.0"}}, "defaultPort": 8000, "defaultExternalPort": 8756, "banner": null}, {"id": "tabby-starcoder-1b", "serviceType": "binary", "version": "1", "name": "Tabby StarCoder 1B", "description": "", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\n## \ud83d\udcbb Hardware Requirements\n\n## \ud83d\udcd2 Example Usage\n\n<img width=\"463\" alt=\"Screenshot 2023-08-01 at 22 46 39\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/04188ee4-9fbc-4d2d-9c3a-2aca359a92e3\">\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\udc2f Getting Started with Tabby VSCode Extension\n\n1. Install Tabby VSCode Extension <a href='https://marketplace.visualstudio.com/items?itemName=TabbyML.vscode-tabby' target='_blank'>here</a>\n\n<img width=\"1207\" alt=\"Screenshot 2023-08-01 at 12 27 29\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/05a85487-b2ad-4bcb-89fb-d610398b2f72\">\n\n2. Enable the extension and provide the URL where the service is running\n\n<img width=\"1180\" alt=\"Screenshot 2023-08-01 at 12 28 07\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/258eaa26-78cd-41a8-a4f1-c4924f0696aa\">\n\n3. Wait a few minutes\n\n> The model will be downloaded when the server starts. For this reason, you will need a few minutes before the extension starts to work properly.\n\n### \ud83d\udd0e Quality Benchmarks\n\nThe model has been evaluated on two code generation benchmarks: HumanEval and MTPB. Please refer to the <a href='https://arxiv.org/abs/2203.13474' target='_blank'>paper</a> for more details.\n\n### \ud83d\udeab Limitations and Biases\n\n\n## \ud83d\udcdc License\n\nThe model is under the 3-Clause BSD license.\n", "beta": true, "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/coder-tabby-starcoder-1b/logo.svg", "modelInfo": {"memoryRequirements": 3204}, "interfaces": ["coder"], "defaultPort": 8000, "defaultExternalPort": 8468, "weightsDirectoryUrl": "https://huggingface.co/TabbyML/StarCoder-1B/resolve/main/", "weightsFiles": ["tabby.json", "tokenizer.json", "ggml/q8_0.gguf"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/TabbyML/tabby/releases/download/v0.3.0/tabby_aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null}, "serveCommand": "tabby_aarch64-apple-darwin serve --model . --device metal --port 8468", "banner": null}, {"id": "weaviate", "name": "Weaviate", "description": "Weaviate is an open-source, cloud-native vector database designed to enable machine learning (ML) and artificial intelligence (AI) capabilities for your data. It's built to handle large-scale data storage and search operations, making it a powerful tool for data scientists and developers working with big data [Learn More](https://weaviate.io/developers/weaviate)", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\n<a href='https://weaviate.io/' target='_blank'>Weaviate</a> is an open-source, cloud-native vector database designed to enable machine learning (ML) and artificial intelligence (AI) capabilities for your data. It's built to handle large-scale data storage and search operations, making it a powerful tool for data scientists and developers working with big data <a href='https://weaviate.io/developers/weaviate' target='_blank'>Learn more</a> \ud83d\ude80.\n\n## \ud83d\udc47 Getting Started (Implementation)\n\nThe service can be used with Langchain or the official weavaite python client (https://github.com/qdrant/qdrant). Below you can find an example using the service with Langchain. In the code snippet, we are assuming that you are using all-miniLM-l6-v2 model for embeddings generation and the service is running locally on port 8001.\n\n```python\n\n!pip install weaviate-client\n\nimport os\n\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.docstore.document import Document\nfrom langchain.vectorstores import Weaviate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\ndoc1 = Document(page_content=\"Prem is an easy to use open source AI platform. With Prem you can quickly build provacy preserving AI applications.\")\ndoc2 = Document(page_content=\"\"\"\nPrem App\n\nAn intuitive desktop application designed to effortlessly deploy and self-host Open-Source AI models without exposing sensitive data to third-party.\n\n\"\"\")\ndoc3 = Document(page_content=\"\"\"\nPrem Benefits\n\nEffortless Integration\nSeamlessly implement machine learning models with the user-friendly interface of OpenAI's API.\n\nReady for the Real World\nBypass the complexities of inference optimizations. Prem's got you covered.\n\nRapid Iterations, Instant Results\nDevelop, test, and deploy your models in just minutes.\n\nPrivacy Above All\nYour keys, your models. We ensure end-to-end encryption.\n\nComprehensive Documentation\nDive into our rich resources and learn how to make the most of Prem.\n\nPreserve Your Anonymity\nMake payments with Bitcoin and Cryptocurrency. It's a permissionless infrastructure, designed for you.\n\"\"\")\n\n# Using sentence transformers all-MiniLM-L6-v2\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/v1\")\n\n# Using locally running Weaviate\nurl = \"http://localhost:8080\"\n\nvectorstore = Weaviate.from_documents(\n    [doc1, doc2, doc3], \n    embeddings, \n    weaviate_url=url, \n    by_text=False,\n)\n\nquery = \"What are Prem Benefits?\"\ndocs = vectorstore.similarity_search(query)\nprint(docs[0].page_content)\n```", "interfaces": ["vector-store"], "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/vector-store-weaviate/logo.svg", "modelInfo": {}, "envVariables": ["QUERY_DEFAULTS_LIMIT=25", "AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true", "PERSISTENCE_DATA_PATH=/var/lib/weaviate", "DEFAULT_VECTORIZER_MODULE=none", "CLUSTER_HOSTNAME=node1"], "volumePath": "/var/lib/weaviate", "dockerImages": {"cpu": {"size": 46718418, "image": "semitechnologies/weaviate:1.19.11"}}, "defaultPort": 8080, "defaultExternalPort": 8080, "banner": null}, {"id": "mpt-7b-instruct", "name": "MPT 7B Instruct", "beta": true, "description": "MPT-7B-Instruct is a model for short-form instruction following. It's built by finetuning MPT-7B on a dataset derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets. Its inference can be run on various GPU configurations. It's released under cc-by-sa-3.0 which enables commercial usage with proper credits.", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\nMPT-7B-Instruct is a model for short-form instruction following. It is built by finetuning  <a href='https://huggingface.co/mosaicml/mpt-7b' target='_blank'>MPT-7B</a> on a <a href='https://huggingface.co/datasets/sam-mosaic/dolly_hhrlhf' target='_blank'>dataset</a> derived from the <a href='https://huggingface.co/datasets/databricks/databricks-dolly-15k' target='_blank'>Databricks Dolly-15k</a> and the  <a href='https://huggingface.co/datasets/Anthropic/hh-rlhf' target='_blank'>Anthropic Helpful and Harmless (HH-RLHF)</a> datasets.\n\n## \ud83d\udcbb Hardware Requirements\n> **Memory requirements**: 14.1 GB (13455 MiB).\n\nTo run the `mpt-7b-instruct` service, you'll need the following hardware configuration:\n\n### Cloud Platforms\n\nIf you are using AWS:\n\n- Instance Type: `p3.2xlarge` or higher\n- GPU: NVIDIA V100 or higher.\n\nIf you are using Paperspace:\n\n- Instance Type: `V100` or higher\n- GPU: NVIDIA V100 or higher.\n\n### On-Premise Platforms\n\nYou'll need access to a GPU with the following options:\n\n- V100 GPUs: The model can be run on V100 GPUs.\n\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> \\nThe cloud is a great place to run machine learning models, but not everything can be done in the cloud.\\nIn this post, I\u2019ll explain why you might need to run machine learning models on-premise, and how you can do it.\\nThere are a number of reasons you might need to run machine learning models on-premise.\\nThe first is security. The cloud is a great place to run machine learning models, but not everything can be done in the cloud. For example, you might have sensitive data that you don\u2019t\n\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> \\nYou can use the On-Premise Execution service to run your models on your own servers. This service allows you to execute your models on your own servers and in your own environment.\\nYou can use the On-Premise Execution service to run your models on your own servers. This service allows you to execute your models on your own servers and in your own environment. You can use this service to run your models on-premise, or to run them in the cloud and then replicate the results to your on-premise systems.\\nTo use the On\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> \\nThere are a few limitations to running your models on-premise.\\nThe first limitation is the amount of data that can be processed. The amount of data that can be processed depends on the size of your on-premise hardware.\\nThe second limitation is the time it takes to process the data. The time it takes to process the data depends on the amount of data and the model\u2019s complexity.\\nThe third limitation is the speed of your internet connection. The internet connection needs to be fast enough to send the results of your model back to your on\n\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n<blockquote>\n\n\\nDeploying a model on-premise allows you to have more control over your data and processes, but it can be more expensive and time-consuming to set up. If you\u2019re considering an on-premise deployment, it\u2019s important to understand the trade-offs and plan accordingly.\\nIf you\u2019re considering an on-premise deployment, it\u2019s important to understand the trade-offs and plan accordingly.The best way to learn how to use a new technology is to try it out.The best way to learn how to use a new technology is to try it out.\\nIn the world of data science, the term \u201cdata lake\u201d refers to a large repository of raw data that is stored in a single location.\\nIn the world of data science, the term \u201cdata lake\u201d refers to a large repository of raw data that is stored in a single location. Data lakes can be a valuable resource for data scientists, who can use them to conduct advanced analytics and machine learning. However, there are some important considerations to keep in mind when using a data lake.\\nOne of the most important considerations is security. Data lakes\n\n</blockquote>\n\nIt's visible from the above outputs that model doesn't answer in question answer mode by default and it's really bad at knowing when to stop. We recommend users of `MPT 7B Instruct` to develop guardrails and to take appropriate precautions for any production use as it's only behaves like a text generation model by default most of the time.\n\nAn example would be using a Chat Prompt Template as shown below but it doesn't work well all the time even with it:\n\nPrompt:\n```\nYou are an AI assistant in a conversational setting.\nProvide a concise and accurate conversational answer to anything User asks.\n===================\n\nUser: What are the trade-offs of deploying models on-premise I should be aware of?\nAssistant:\"\"\"\n```\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8452/v1\", max_tokens=128)\nmessages = [HumanMessage(content=\"Why do I need to run machine learning models on-premise?\")]\nprint(chat(messages))\n\n# output:\n# \\nWhen it comes to machine learning, it\u2019s important to understand that there are two types of models: supervised and unsupervised.\\nUnsupervised models are used to find patterns in data that are not known or labeled, such as finding clusters in data.\\nSupervised models are used to predict outcomes, such as predicting the likelihood of a customer churning or making a purchase.\\nTo train a supervised model, you need labeled data. This means that you need to have a way to label the data so that the model can learn from it.\\nIn\n```\n\nFor using it in a chat setting we recommend using a Chat Prompt Template as shown below:\n    \n```python\n\nimport os\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat_template = \"\"\"\nYou are an AI assistant in a conversational setting.\nProvide a conversational answer to any question an User asks. Be original, concise, accurate and helpful.\n===================\n\nUser: {user_message}\nAssistant:\"\"\"\nprompt = PromptTemplate(\n    input_variables=[\"user_message\"],\n    template=chat_template,\n)\n\nuser_message = \"Why do I need to run machine learning models on-premise?\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8452/v1\", max_tokens=128)\nchain = LLMChain(llm=chat, prompt=prompt, verbose=True)\nprint(chain.run(user_message=user_message))\n\n# output: > Finished chain.\n# Machine learning models are usually trained on large amounts of data and it can be challenging to get all the data required for training in a single location. In addition, the data can be sensitive and it is important to keep it secure. Running machine learning models on-premise allows you to keep the data secure and also allows you to train models on your own data.\\n#\n\n```\n---\n##### This model was trained on data formatted in the dolly-15k format:\n\n```python\nINSTRUCTION_KEY = \"### Instruction:\"\nRESPONSE_KEY = \"### Response:\"\nINTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\nPROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n{instruction_key}\n{instruction}\n{response_key}\n\"\"\".format(\n    intro=INTRO_BLURB,\n    instruction_key=INSTRUCTION_KEY,\n    instruction=\"{instruction}\",\n    response_key=RESPONSE_KEY,\n)\n\nexample = \"James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week? Explain before answering.\"\nfmt_ex = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)\n# fmt_ex is ready to be tokenized and sent through the model.\n```\n\n### \ud83d\udd0e Quality Benchmarks\n\n\n### \ud83d\udeab Limitations and Biases\nMPT-7B-Instruct can produce factually incorrect output, and should not be relied on to produce factually accurate information. MPT-7B-Instruct was trained on various public datasets. While great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\nWe recommend users of MPT-7B-Instruct to develop guardrails, use it with above prompt templates and to take appropriate precautions while using it.\n\n\n## \ud83d\udcdc License\nIt's released under CC-By-SA-3.0 which enables commercial usage with proper credits given.", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/chat-mpt-7b-instruct/logo.svg", "modelInfo": {"memoryRequirements": 13455, "tokensPerSecond": 40}, "interfaces": ["chat"], "dockerImages": {"gpu": {"size": 33040731339, "image": "ghcr.io/premai-io/mpt-7b-instruct-gpu:1.0.0"}}, "defaultPort": 8000, "defaultExternalPort": 8452, "banner": null}, {"id": "stable-beluga-2", "serviceType": "binary", "version": "1", "name": "Stable Beluga 2", "description": "Stable Beluga 2 is a Llama2 70B model finetuned on an Orca style Dataset.", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\nStable Beluga 2 is a specialized model built upon the Llama2 70B architecture and fine-tuned using an Orca Style Dataset. This model is available for free testing through Stable Chat's Research Preview platform, aimed at showcasing Stability AI's most advanced language models.\n\n## \ud83d\udcd2 Example Usage\n\n### 1\ufe0f\u20e3 Prompt: Why do I need to run machine learning models on-premise?\n\n> The cloud is a great place to run machine learning models, but not everything can be done in the cloud.\n> In this post, I\u2019ll explain why you might need to run machine learning models on-premise, and how you can do it.\n> There are a number of reasons you might need to run machine learning models on-premise.\n> The first is security. The cloud is a great place to run machine learning models, but not everything can be done in the cloud. For example, you might have sensitive data that you don\u2019t\n\n\n### 2\ufe0f\u20e3 Prompt: How do I run my models on-premise?\n\n> You can use the On-Premise Execution service to run your models on your own servers. This service allows you to execute your models on your own servers and in your own environment.\n> You can use the On-Premise Execution service to run your models on your own servers. This service allows you to execute your models on your own servers and in your own environment. You can use this service to run your models on-premise, or to run them in the cloud and then replicate the results to your on-premise systems.\n> To use the On\n\n### 3\ufe0f\u20e3 Prompt: What are the limitations of running my models on-premise?\n\n> There are a few limitations to running your models on-premise.\n> The first limitation is the amount of data that can be processed. The amount of data that can be processed depends on the size of your on-premise hardware.\n> The second limitation is the time it takes to process the data. The time it takes to process the data depends on the amount of data and the model\u2019s complexity.\n> The third limitation is the speed of your internet connection. The internet connection needs to be fast enough to send the results of your model back to your on\n\n\n### 4\ufe0f\u20e3 Prompt: What are the trade-offs of deploying models on-premise I should be aware of?\n\n> Deploying a model on-premise allows you to have more control over your data and processes, but it can be more expensive and time-consuming to set up. If you\u2019re considering an on-premise deployment, it\u2019s important to understand the trade-offs and plan accordingly.\n> If you\u2019re considering an on-premise deployment, it\u2019s important to understand the trade-offs and plan accordingly.The best way to learn how to use a new technology is to try it out.The best way to learn how to use a new technology is to try it out.\n> In the world of data science, the term \u201cdata lake\u201d refers to a large repository of raw data that is stored in a single location.\n> In the world of data science, the term \u201cdata lake\u201d refers to a large repository of raw data that is stored in a single location. Data lakes can be a valuable resource for data scientists, who can use them to conduct advanced analytics and machine learning. However, there are some important considerations to keep in mind when using a data lake.\n> One of the most important considerations is security. Data lakes\n\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83e\udd9c\ud83d\udd17 Getting Started with Langchain\n\n```bash\npip install langchain openai\n```\n\nIt can be run simply using the langchain library as shown below:\n\n```python\nimport os\nfrom langchain.schema import HumanMessage\nfrom langchain.chat_models import ChatOpenAI\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nchat = ChatOpenAI(openai_api_base=\"http://localhost:8734/v1\", max_tokens=512)\nmessages = [HumanMessage(content=\"Why do I need to run machine learning models on-premise?\")]\nprint(chat(messages))\n\n# output:\n# \\nWhen it comes to machine learning, it\u2019s important to understand that there are two types of models: supervised and unsupervised.\\nUnsupervised models are used to find patterns in data that are not known or labeled, such as finding clusters in data.\\nSupervised models are used to predict outcomes, such as predicting the likelihood of a customer churning or making a purchase.\\nTo train a supervised model, you need labeled data. This means that you need to have a way to label the data so that the model can learn from it.\\nIn\n```\n\n### \ud83d\udd0e Quality Benchmarks\n\n### \ud83d\udeab Limitations and Biases\n\nBeluga is an emerging technology with inherent risks. Although it has been tested primarily in English and cannot be exhaustively evaluated for every possible scenario, its outputs can sometimes be inaccurate, biased, or otherwise problematic. Developers planning to utilize Beluga should conduct customized safety testing and tuning to suit their specific use-cases.\n\n## \ud83d\udcdc License\nIt is made available under a custom commercial license, which is available <a href='https://ai.meta.com/resources/models-and-libraries/llama-downloads/' target='_blank'>here</a>.", "beta": true, "petals": true, "petals_id": "StableBeluga2", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/chat-stable-beluga-2/logo.svg", "modelInfo": {"memoryRequirements": 4096, "tokensPerSecond": "Variable"}, "interfaces": ["chat"], "defaultPort": 8734, "defaultExternalPort": 8734, "weightsDirectoryUrl": "https://huggingface.co/petals-team/StableBeluga2/resolve/main/", "weightsFiles": ["config.json", "model.safetensors.index.json", "special_tokens_map.json", "tokenizer.model", "generation_config.json", "model_00081-of-00081.safetensors", "tokenizer.json", "tokenizer_config.json"], "binariesUrl": {"aarch64-apple-darwin": null, "x86_64-apple-darwin": null, "universal-apple-darwin": "https://raw.githubusercontent.com/premAI-io/prem-services/main/cht-petals/setup-petals.sh"}, "serveCommand": "setup-petals.sh --model-id petals-team/StableBeluga2 --model-path . --dht-prefix StableBeluga2-hf --port 8734", "banner": null}, {"id": "all-minilm-l6-v2", "name": "All MiniLM L6 v2", "description": "All-MiniLM-L6-v2 is a sentence-transformers model designed to map sentences and paragraphs to a 384-dimensional dense vector space, ideal for clustering or semantic search tasks. Developed during Hugging Face's Community week, this model is fine-tuned on a 1B sentence pairs dataset with a contrastive learning objective. It excels in encoding short texts, capturing semantic information, and is useful for information retrieval, clustering, or sentence similarity tasks. [Learn More](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)", "documentation": "# \ud83d\udcd1 Documentation\n\n## \ud83d\udccc Description\n\nAll-MiniLM-L6-v2 is a sentence-transformers model designed to map sentences and paragraphs to a 384-dimensional dense vector space, ideal for clustering or semantic search tasks. Developed during <a href='https://huggingface.co/' target='_blank'>Hugging Face</a>'s Community week, this model is fine-tuned on a 1B sentence pairs dataset with a contrastive learning objective. It excels in encoding short texts, capturing semantic information, and is useful for information retrieval, clustering, or sentence similarity tasks. <a href='https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2' target='_blank'>Learn more</a> \ud83d\ude80.\n\n## \ud83d\udc47 Getting Started\n\nThe service is compatible with \ud83e\udd9c\ud83d\udd17<a href='https://github.com/hwchase17/langchain' target='_blank'>LangChain</a> and follows OpenAI <a href='https://platform.openai.com/docs/api-reference' target='_blank'>API request-response</a> format. If you haven't already, you will need to install :\n\n* `langchain` \u27a1\ufe0f <a href='https://pypi.org/project/langchain/' target='_blank'>pip install</a>.\n* `tiktoken` \u27a1\ufe0f <a href='https://pypi.org/project/tiktoken/' target='_blank'>pip install</a>.\n* `openai` \u27a1\ufe0f <a href='https://pypi.org/project/openai/' target='_blank'>pip install</a>.\n\n## \u2692\ufe0f Usage\n\n\ud83d\udc49 Find an example for using the service with \ud83e\udd9c\ud83d\udd17 LangChain below:\n\n```python\nimport os\n\nfrom langchain.embeddings import OpenAIEmbeddings\n\nos.environ[\"OPENAI_API_KEY\"] = \"random-string\"\n\nembeddings = OpenAIEmbeddings(openai_api_base=\"http://localhost:8444/v1\")\ntext = \"Prem is an easy to use open source AI platform.\"\nquery_result = embeddings.embed_query(text)\ndoc_result = embeddings.embed_documents([text])\n```\n\n\nAlso check the official sentence transformers <a href='https://www.sbert.net/' target='_blank'>documentation</a>. It provides extensive examples and detailed information for using the model.\n\n## \ud83d\udc40 Intended Uses\nThe model is meant to be used as an encoder for single sentences and short paragraphs. Given an input text, it outputs a vector that captures the semantic information. You can use the sentence vector generated for information retrieval, clustering,\u00a0or sentence similarity tasks.\n\nBy default, input text longer than 256-word\u00a0pieces is truncated.\n\n## \ud83d\udd0e Evaluation Results\nFor an automated evaluation of this model, see the Sentence Embeddings Benchmark <a href='https://seb.sbert.net' target='_blank'>page</a>.\n\n## \u2696\ufe0f License\n\nThe model is published under <a href='https://www.apache.org/licenses/LICENSE-2.0' target='_blank'>Apache License 2.0</a>.\n", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/embeddings-all-minilm-l6-v2/logo.svg", "modelInfo": {"memoryRequirements": 8192}, "dockerImages": {"cpu": {"size": 1238261448, "image": "ghcr.io/premai-io/embeddings-all-minilm-l6-v2-cpu:1.0.3"}, "gpu": {"size": 20687072077, "image": "ghcr.io/premai-io/embeddings-all-minilm-l6-v2-gpu:1.0.3"}}, "interfaces": ["embeddings"], "defaultPort": 8000, "defaultExternalPort": 8444, "banner": null}, {"id": "whisper-tiny-cpp", "name": "Whisper Tiny CPP", "beta": true, "description": "", "documentation": "# Documentation\n\n## \ud83d\udccc Description\n\nWhisper Tiny is a compact version of OpenAI's Whisper model, designed for automatic speech recognition (ASR) and speech translation. Despite its smaller size, it retains the powerful capabilities of the larger models, making it suitable for applications where computational resources or storage space are limited. <a href='https://huggingface.co/openai/whisper-tiny' target='_blank'>Learn More</a>.\n\n## \ud83d\udcbb Hardware Requirements\n\nTo run the `whisper-tiny-cpp` service on Prem, you'll just need a CPU with at least 4GiB of RAM.\n\n## \ud83d\udcd2 Example Usage\n\nWhisper Tiny can be used for various tasks, including English to English transcription, French to French transcription, and French to English translation. It can also handle long-form transcription by using a chunking algorithm, allowing it to transcribe audio samples of arbitrary length.\n\n### \ud83c\udfb6 sample.wav. You can find the file [here](https://github.com/premAI-io/prem-registry/blob/main/audio-to-text-whisper-tiny/sample.wav)\n\n<img width=\"1449\" alt=\"image\" src=\"https://github.com/premAI-io/prem-registry/assets/29598954/4b879d6b-4404-47ae-b3c9-f2f5fd38ec0e\">\n\n## \ud83d\udee0\ufe0f Technical Details\n\n### \ud83d\ude80 Getting Started with OpenAI Python client\n\nThe service exposes the same endpoints as OpenAI DALL-E does. You can directly use the official `openai` python library.\n\n```python\n\n!pip install openai\n\nimport openai\n\nopenai.api_base = \"http://184.105.5.51:10111/v1\"\nopenai.api_key = \"random-string\"\n\naudio_file = open(\"./sample.wav\", \"rb\")\ntranscript = openai.Audio.transcribe(\"whisper-1\", audio_file)\nprint(transcript)\n\n```\n\n## \ud83d\udcdc License\n\nWhisper's code and model weights are released under the MIT License.\n", "serviceType": "binary", "version": "1", "icon": "https://raw.githubusercontent.com/premAI-io/prem-registry/v1/audio-to-text-whisper-tiny-cpp/logo.svg", "modelInfo": {"memoryRequirements": 4800}, "interfaces": ["audio-to-text"], "defaultExternalPort": 9446, "weightsDirectoryUrl": "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/", "weightsFiles": ["ggml-tiny.bin"], "binariesUrl": {"aarch64-apple-darwin": "https://github.com/tiero/whisperd/releases/download/v0.1.12/whisperd-aarch64-apple-darwin", "x86_64-apple-darwin": null, "universal-apple-darwin": null, "x86_64-unknown-linux-gnu": "https://github.com/tiero/whisperd/releases/download/v0.1.12/whisperd-x86_64-unknown-linux-gnu"}, "serveCommand": "whisperd-aarch64-apple-darwin serve  --model-path=ggml-tiny.bin --port=9446", "banner": null}]